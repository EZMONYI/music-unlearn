2024-12-28 18:27:52 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='runs', seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir='mass', empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, checkpoint_suffix='', checkpoint_shard_count=1, quantization_config_path=None, profile=False, criterion='label_smoothed_cross_entropy_with_align', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='xmasked_seq2seq', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, distributed_wrapper='DDP', slowmo_momentum=None, slowmo_algorithm='LocalSGD', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', arch='xtransformer', max_epoch=20, max_update=2000000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[5e-05], min_lr=1e-09, use_bmuf=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, label_smoothing=0.0, attn_loss_weight=1.0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, warmup_updates=4000, warmup_init_lr=1e-07, data='lmd_data/processed', langs='lyric,melody', source_langs='lyric,melody', target_langs='lyric,melody', valid_lang_pairs='lyric-lyric,melody-melody', mass_steps='lyric-lyric,melody-melody', mt_steps='lyric-melody,melody-lyric', raw_text=False, lazy_load=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, source_lang=None, target_lang=None, lm_bias=False, word_mask=0.25, word_mask_keep_rand='0.8,0.1,0.1', reload_checkpoint=None, dropout=0.1, activation_dropout=0.1, attention_dropout=0.1, share_decoder_input_output_embed=True, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, adaptive_input=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, distributed_num_procs=1)
| [lyric] dictionary: 21904 types
| [melody] dictionary: 152 types
2024-12-28 18:27:52 | INFO | fairseq.data.data_utils | loaded 679 examples from: lmd_data/processed/valid.lyric
| monolingual valid-lyric: 679 examples
2024-12-28 18:27:52 | INFO | fairseq.data.data_utils | loaded 679 examples from: lmd_data/processed/valid.melody
| monolingual valid-melody: 679 examples
2024-12-28 18:27:52 | INFO | fairseq.data.data_utils | loaded 679 examples from: lmd_data/processed/valid.lyric-melody.lyric
2024-12-28 18:27:52 | INFO | fairseq.data.data_utils | loaded 679 examples from: lmd_data/processed/valid.lyric-melody.melody
| bilingual valid lyric-melody.lyric: 679 examples
| bilingual valid lyric-melody.melody: 679 examples
2024-12-28 18:27:54 | INFO | fairseq_cli.train | ModuleList(
  (0-1): 2 x XTransformerModel(
    (encoders): ModuleDict(
      (lyric): XTransformerEncoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(21904, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0-5): 6 x TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout_module): FairseqDropout()
            (activation_dropout_module): FairseqDropout()
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (melody): XTransformerEncoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(152, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0-5): 6 x TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (dropout_module): FairseqDropout()
            (activation_dropout_module): FairseqDropout()
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (decoders): ModuleDict(
      (lyric): XTransformerDecoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(21904, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0-5): 6 x MaskedAttentionDecoderLayer(
            (dropout_module): FairseqDropout()
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_dropout_module): FairseqDropout()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): MaskedMHAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (output_projection): Linear(in_features=512, out_features=21904, bias=False)
      )
      (melody): XTransformerDecoder(
        (dropout_module): FairseqDropout()
        (embed_tokens): Embedding(152, 512, padding_idx=1)
        (embed_positions): SinusoidalPositionalEmbedding()
        (layers): ModuleList(
          (0-5): 6 x MaskedAttentionDecoderLayer(
            (dropout_module): FairseqDropout()
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_dropout_module): FairseqDropout()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): MaskedMHAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (output_projection): Linear(in_features=512, out_features=152, bias=False)
      )
    )
  )
)
2024-12-28 18:27:54 | INFO | fairseq_cli.train | task: xmasked_seq2seq (XMassTranslationTask)
2024-12-28 18:27:54 | INFO | fairseq_cli.train | model: xtransformer (ModuleList)
2024-12-28 18:27:54 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_with_align (LabelSmoothedCrossEntropyCriterionWithAlign)
2024-12-28 18:27:54 | INFO | fairseq_cli.train | num. model params: 199139328 (num. trained: 199139328)
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 0.encoders.lyric.embed_tokens.weight <- 0.decoders.lyric.embed_tokens.weight
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 0.encoders.lyric.embed_tokens.weight <- 0.decoders.lyric.output_projection.weight
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 0.encoders.melody.embed_tokens.weight <- 0.decoders.melody.embed_tokens.weight
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 0.encoders.melody.embed_tokens.weight <- 0.decoders.melody.output_projection.weight
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 0.decoders.lyric.output_projection.bias <- 0.decoders.melody.output_projection.bias
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 0.decoders.lyric.output_projection.bias <- 1.decoders.lyric.output_projection.bias
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 0.decoders.lyric.output_projection.bias <- 1.decoders.melody.output_projection.bias
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 1.encoders.lyric.embed_tokens.weight <- 1.decoders.lyric.embed_tokens.weight
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 1.encoders.lyric.embed_tokens.weight <- 1.decoders.lyric.output_projection.weight
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 1.encoders.melody.embed_tokens.weight <- 1.decoders.melody.embed_tokens.weight
2024-12-28 18:27:54 | INFO | fairseq.trainer | detected shared parameter: 1.encoders.melody.embed_tokens.weight <- 1.decoders.melody.output_projection.weight
2024-12-28 18:27:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-12-28 18:27:54 | INFO | fairseq.utils | rank   0: capabilities =  9.0  ; total memory = 79.109 GB ; name = NVIDIA H800                             
2024-12-28 18:27:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-12-28 18:27:54 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-12-28 18:27:54 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
/home/yi/projects/songmass/fairseq-0.10.2/fairseq/checkpoint_utils.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
Traceback (most recent call last):
  File "/home/yi/projects/songmass/fairseq-0.10.2/fairseq/trainer.py", line 290, in load_checkpoint
    self.get_model().load_state_dict(
TypeError: load_state_dict() got an unexpected keyword argument 'args'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yi/miniconda3/envs/unlearn/bin/fairseq-train", line 8, in <module>
    sys.exit(cli_main())
  File "/home/yi/projects/songmass/fairseq-0.10.2/fairseq_cli/train.py", line 352, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/yi/projects/songmass/fairseq-0.10.2/fairseq/distributed_utils.py", line 301, in call_main
    main(args, **kwargs)
  File "/home/yi/projects/songmass/fairseq-0.10.2/fairseq_cli/train.py", line 110, in main
    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(
  File "/home/yi/projects/songmass/fairseq-0.10.2/fairseq/checkpoint_utils.py", line 188, in load_checkpoint
    extra_state = trainer.load_checkpoint(
  File "/home/yi/projects/songmass/fairseq-0.10.2/fairseq/trainer.py", line 298, in load_checkpoint
    raise Exception(
Exception: Cannot load model parameters from checkpoint checkpoints/checkpoint_last.pt; please ensure that the architectures match.
